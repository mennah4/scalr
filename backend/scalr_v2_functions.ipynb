{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: pyspark in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (3.5.1)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: delta in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (0.4.2)\n",
      "Collecting scikit-learn (from -r requirements.txt (line 5))\n",
      "  Using cached scikit_learn-1.4.1.post1-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting statsmodels (from -r requirements.txt (line 6))\n",
      "  Using cached statsmodels-0.14.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.5 kB)\n",
      "Collecting scipy (from -r requirements.txt (line 7))\n",
      "  Using cached scipy-1.13.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "Collecting matplotlib (from -r requirements.txt (line 8))\n",
      "  Using cached matplotlib-3.8.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mennahjafar/Library/Python/3.11/lib/python/site-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pyspark->-r requirements.txt (line 2)) (0.10.9.7)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->-r requirements.txt (line 5))\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->-r requirements.txt (line 5))\n",
      "  Using cached threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting patsy>=0.5.4 (from statsmodels->-r requirements.txt (line 6))\n",
      "  Using cached patsy-0.5.6-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/mennahjafar/Library/Python/3.11/lib/python/site-packages (from statsmodels->-r requirements.txt (line 6)) (24.0)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->-r requirements.txt (line 8))\n",
      "  Using cached contourpy-1.2.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 8))\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->-r requirements.txt (line 8))\n",
      "  Using cached fonttools-4.51.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (159 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->-r requirements.txt (line 8))\n",
      "  Using cached kiwisolver-1.4.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting pillow>=8 (from matplotlib->-r requirements.txt (line 8))\n",
      "  Using cached pillow-10.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->-r requirements.txt (line 8))\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: six in /Users/mennahjafar/Library/Python/3.11/lib/python/site-packages (from patsy>=0.5.4->statsmodels->-r requirements.txt (line 6)) (1.16.0)\n",
      "Using cached scikit_learn-1.4.1.post1-cp311-cp311-macosx_12_0_arm64.whl (10.4 MB)\n",
      "Using cached statsmodels-0.14.1-cp311-cp311-macosx_11_0_arm64.whl (10.1 MB)\n",
      "Using cached scipy-1.13.0-cp311-cp311-macosx_12_0_arm64.whl (30.3 MB)\n",
      "Using cached matplotlib-3.8.4-cp311-cp311-macosx_11_0_arm64.whl (7.5 MB)\n",
      "Using cached contourpy-1.2.1-cp311-cp311-macosx_11_0_arm64.whl (245 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.51.0-cp311-cp311-macosx_10_9_universal2.whl (2.8 MB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Using cached kiwisolver-1.4.5-cp311-cp311-macosx_11_0_arm64.whl (66 kB)\n",
      "Using cached patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "Using cached pillow-10.3.0-cp311-cp311-macosx_11_0_arm64.whl (3.4 MB)\n",
      "Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Using cached threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, pyparsing, pillow, patsy, kiwisolver, joblib, fonttools, cycler, contourpy, scikit-learn, matplotlib, statsmodels\n",
      "Successfully installed contourpy-1.2.1 cycler-0.12.1 fonttools-4.51.0 joblib-1.3.2 kiwisolver-1.4.5 matplotlib-3.8.4 patsy-0.5.6 pillow-10.3.0 pyparsing-3.1.2 scikit-learn-1.4.1.post1 scipy-1.13.0 statsmodels-0.14.1 threadpoolctl-3.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6d81561-43b0-4f8d-8ffe-844089d9b3eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from functools import reduce\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import delta\n",
    "import numpy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7105e86-9a73-4fe2-b5b8-884c3ca018a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# r-squared of two series\n",
    "\n",
    "def r_squared(series_1, series_2):\n",
    "    corr_matrix = numpy.corrcoef(series_1, series_2)\n",
    "    corr = corr_matrix[0,1]\n",
    "    R_sq = corr**2\n",
    "\n",
    "    return R_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae6646c9-c67e-4ef7-a62f-32ccd47ff24f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# r-squared of each column in a df to every other column\n",
    "\n",
    "def r_squared_metrics(list_of_columns, df):\n",
    "    r_squared_dict = {}\n",
    "    for i in list_of_columns:\n",
    "        series_i = [row[i] for row in df.select(col(i).cast('int')).collect()]\n",
    "        \n",
    "        for j in list_of_columns:\n",
    "            series_j = [row[j] for row in df.select(col(j).cast('int')).collect()]\n",
    "            \n",
    "            if i != j:  # Avoid applying the function to the same element with itself\n",
    "                label = f\"r_{i}_{j}\"  # Constructing the label\n",
    "                mirror_label = f\"r_{j}_{i}\" \n",
    "                if mirror_label not in r_squared_dict:\n",
    "                    r_squared_dict[label] = r_squared(series_i, series_j)\n",
    "\n",
    "    return r_squared_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c104bd94-3dd9-4596-bb98-075d15791f72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def split_by_middle_underscore(string_list):\n",
    "    split_strings = []\n",
    "    for s in string_list:\n",
    "        parts = s.split('_')\n",
    "        # Find the index to split on\n",
    "        middle = len(parts) // 2 - 1\n",
    "        # Split the string into two parts\n",
    "        first_part = '_'.join(parts[:middle + 1])\n",
    "        second_part = '_'.join(parts[middle + 1:])\n",
    "        split_strings.append((first_part, second_part))\n",
    "    return split_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e120f07-fc1b-4090-b64a-bf4a07a5d4da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def r_squared_metrics_current_iteration(list_of_columns, df):\n",
    "    r_squared_dict = {}\n",
    "    for i in list_of_columns:\n",
    "        if '_' in i:  # Check if the column name contains an underscore\n",
    "            series_i = [row[i] for row in df.select(col(i).cast('double')).collect()]\n",
    "            \n",
    "            for j in list_of_columns:\n",
    "                if i != j:  # Check if the column name contains an underscore and is different from i\n",
    "                    series_j = [row[j] for row in df.select(col(j).cast('double')).collect()]\n",
    "                    label = f\"r_{i}_{j}\"  # Constructing the label\n",
    "                    mirror_label = f\"r_{j}_{i}\" \n",
    "                    if mirror_label not in r_squared_dict:\n",
    "                        r_squared_dict[label] = r_squared(series_i, series_j)\n",
    "\n",
    "    return r_squared_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1abb348f-55c7-40b0-a051-18242efaf32b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# split a dictionary into values above and below a certain threshold\n",
    "\n",
    "def split_dict_by_threshold(threshold, dictionary):\n",
    "    # Create a new dictionary for values above the threshold\n",
    "    values_above_threshold = {key: value for key, value in dictionary.items() if value > threshold}\n",
    "\n",
    "    # Remove values above the threshold from the original dictionary\n",
    "    original_dictionary = {key: value for key, value in dictionary.items() if value <= threshold}\n",
    "\n",
    "    return(original_dictionary, values_above_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "697600a3-be8d-419a-bab9-e46b0edd430f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# multinomial logistical regression with maximum likelihood estimation\n",
    "# look into using sci-kit learn package\n",
    "\n",
    "def multi_log_reg_w_mle(df, dependent_variable):\n",
    "\n",
    "    final_pandas_df = df.toPandas()\n",
    "    final_pandas_df = final_pandas_df.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Assume 'df' is your DataFrame, 'Y' is the dependent variable, and the rest are independent variables \n",
    "    X = final_pandas_df.drop(dependent_variable, axis=1)\n",
    "    y = final_pandas_df[dependent_variable]\n",
    "\n",
    "\n",
    "    # Add a constant to the model (if you want an intercept)\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Fit the model\n",
    "    model = sm.MNLogit(y, X)\n",
    "    result = model.fit()\n",
    "\n",
    "    print(result.summary())\n",
    "\n",
    "\n",
    "    # Create the summary dictionary for this dependent variable\n",
    "    summary = {\n",
    "        'coefficients': result.params.to_dict(),\n",
    "        'p_values': result.pvalues.to_dict(),\n",
    "        'AIC': result.aic,\n",
    "        'BIC': result.bic\n",
    "    }\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9683339-7eae-4cba-bdc2-1fde1fcca734",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## alternative to function above\n",
    "\n",
    "def multi_log_reg_sklearn(df, dependent_variable):\n",
    "    # Convert to Pandas DataFrame if it's a Spark DataFrame\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        df = df.toPandas()\n",
    "\n",
    "    # Convert data to numeric, handling errors\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Splitting the dataset into features and target variable\n",
    "    X = df.drop(dependent_variable, axis=1)\n",
    "    y = df[dependent_variable]\n",
    "\n",
    "    # Create the model with multinomial option\n",
    "    model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # You can add additional code to print or return the model summary, coefficients, etc.\n",
    "    coefficients = model.coef_\n",
    "    intercept = model.intercept_\n",
    "\n",
    "    summary = {\n",
    "        'coefficients': coefficients,\n",
    "        'intercept': intercept\n",
    "        # You can add more metrics as needed\n",
    "    }\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Solver: The 'lbfgs' solver is used for multinomial logistic regression. You can experiment with other solvers like 'newton-cg', 'sag', and 'saga'.\n",
    "# Maximum Iterations: max_iter is set to 1000. Depending on your data, you might need to increase this number if the algorithm does not converge.\n",
    "# Return Values: This function returns the coefficients and intercept of the model. You can modify it to return additional metrics or information as needed.\n",
    "# Model Evaluation: Additional code can be added for model evaluation, such as generating a classification report or confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ef0a941-bd1a-4951-9dd2-fd00178d61e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def combine_columns(df, col1, col2):\n",
    "    # Example combination logic: taking the average of two columns\n",
    "    return (col(col1) + col(col2)) / 2"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "scalr_v2_functions",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
